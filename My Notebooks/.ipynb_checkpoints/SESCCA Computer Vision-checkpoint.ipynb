{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "import PIL.Image\n",
    "import os\n",
    "from torchvision.transforms import functional as func\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = 'D:/UDENAR/Electronic Engineering/Ninth Semester/SESCCA/Computer vision results/'\n",
    "image_path = results_path+'Example Images/1.jpg'\n",
    "video_path = results_path+'Example Images/pedestrians_1.mp4'\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader from video frames\n",
    "class FramesDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        if self.transform:\n",
    "            x = self.data[idx]\n",
    "            x = self.transform(x)\n",
    "        return x\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frames(video_path):\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    success, image = vidcap.read()\n",
    "    images_list = []\n",
    "    images_list.append(image)\n",
    "    while success:\n",
    "        success, image = vidcap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        images_list.append(image)\n",
    "    vidcap.release()\n",
    "    return images_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_palette(num_colors):\n",
    "    #Create a color pallette, selecting a color for each class\n",
    "    palette = torch.tensor([2**25-1, 2**15-1, 2**21-1])\n",
    "    colors = torch.as_tensor([i for i in range(num_colors)])[:, None]*palette\n",
    "    colors = (colors%255).numpy().astype('uint8')\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Algorithms\n",
    "## Object Detection\n",
    "### YOLO\n",
    "- Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd D:\\UDENAR\\Electronic Engineering\\Ninth Semester\\SESCCA\\Models\\Object Detection\\yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply YOLO algorithm to an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(image_path)\n",
    "results.save(results_path+'/YOLO Detections')\n",
    "results.pandas().xyxy[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply YOLO algorithm to a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py --source \"D:\\UDENAR\\Electronic Engineering\\Ninth Semester\\SESCCA\\Computer vision results\\Example Images\\pedestrians_1.mp4\" --project \"D:\\UDENAR\\Electronic Engineering\\Ninth Semester\\SESCCA\\Computer vision results\\YOLO Detections\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Real-time detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py --source \"rtsp://admin:EUISDZ@192.168.1.247\" --project \"D:\\UDENAR\\Electronic Engineering\\Ninth Semester\\SESCCA\\Computer vision results\\YOLO Detections\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Segmentation\n",
    "### DeepLabV3 model with a ResNet-101 backbone\n",
    "- Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = torch.hub.load('pytorch/vision:v0.8.0', 'deeplabv3_resnet101', pretrained=True)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "colors = get_color_palette(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply segmentation algorithm to an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = PIL.Image.open(image_path)\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to(device)\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)['out'][0]\n",
    "output_predictions = output.argmax(0)\n",
    "r = PIL.Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)\n",
    "r.putpalette(colors)\n",
    "r = r.convert('RGB')\n",
    "r.save(results_path+f'Semantic Segmentation/output.jpg')\n",
    "r = np.asarray(r)\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "plt.subplot(211)\n",
    "plt.imshow(input_image)\n",
    "plt.subplot(212)\n",
    "plt.imshow(r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply segmentation algorithm to a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidcap = cv2.VideoCapture(video_path)\n",
    "frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half = False\n",
    "half &= device.type != 'cpu'\n",
    "half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "out = cv2.VideoWriter(results_path+'Semantic Segmentation/pedestrian_1.mp4', fourcc, 20.0, (1920, 1080))\n",
    "outputs_list = []\n",
    "for i in range(1, frames-1):\n",
    "    print(i)\n",
    "    success, img = vidcap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    input_tensor = preprocess(img)\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    input_batch = input_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)['out'][0]\n",
    "    output_prediction = output.argmax(0)\n",
    "    r = PIL.Image.fromarray(output_prediction.byte().cpu().numpy()).resize((1920, 1080))\n",
    "    r.putpalette(colors)\n",
    "    r = np.array(r.convert('RGB'))\n",
    "    out.write(r)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Segmentation\n",
    "### Mask R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\OneDrive\\Electronic Engineering\\Ninth Semester\\SESCCA\\Codes\\Me\\SESCCA-Computer-Vision\\My Notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\usuario\\OneDrive\\Electronic Engineering\\Ninth Semester\\SESCCA\\Codes\\Me\\SESCCA-Computer-vision\\My Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(image, targets, device, output=False, th=0.5):\n",
    "    th = torch.tensor([th], device=device)\n",
    "    image = image*255\n",
    "    image = torch.movedim(image.to(torch.uint8), 0, -1)\n",
    "    image = PIL.Image.fromarray(np.array(image.cpu()))\n",
    "    if output:\n",
    "        ids = torch.nonzero(torch.gt(targets['scores'], th))\n",
    "        fields = ['boxes', 'labels', 'scores', 'masks']\n",
    "        targets = {field: targets[field][ids] for field in fields}\n",
    "        targets['masks'] = torch.gt(targets['masks'][:, 0, 0, :, :], th)\n",
    "        targets['masks'] = targets['masks'].to(torch.uint8)\n",
    "        targets['boxes'] = targets['boxes'][:, 0, :]\n",
    "    boxes = targets['boxes'].cpu().to(torch.int)\n",
    "    masks = targets['masks'].cpu()    \n",
    "    \n",
    "    return image, targets, boxes, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(images, targets, idx, device, th=None):\n",
    "    output = True if th!=None else False\n",
    "    img = images[idx]\n",
    "    targets = targets[idx]\n",
    "    img, targets, boxes, masks = preprocess_data(img, targets, device,\n",
    "                                                 output=output, th=th)\n",
    "    num_objs = len(boxes)\n",
    "    color = [int(channel) for channel in torch.tensor([255, 0, 0], device='cuda:0')]\n",
    "    #Add ground truth boxes and masks\n",
    "    for i in range(num_objs):\n",
    "        mask = np.array(masks[i])\n",
    "        contours,_ = cv2.findContours(mask, cv2.RETR_EXTERNAL,\n",
    "                                      cv2.CHAIN_APPROX_NONE)\n",
    "        img = cv2.drawContours(np.array(img), contours, -1, color, 2)\n",
    "        coord = boxes[i]\n",
    "        img = cv2.rectangle(img, (coord[0], coord[1]), (coord[2], coord[3]),\n",
    "                            color, 2)\n",
    "    return targets, img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'D:/UDENAR/Electronic Engineering/Ninth Semester/SESCCA/Models/Instance Segmentation/mask r-cnn 1.1'\n",
    "model = torch.load(model_path)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "mask_transforms = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply Mask-RCNN to an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = PIL.Image.open(image_path).convert('RGB')\n",
    "input_tensor = mask_transforms(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "input_batch = input_batch.to(device)\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pred_img = get_img(input_batch, output, 0, device, th=0.5)\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.imshow(pred_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply Mask-RCNN to a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidcap = cv2.VideoCapture(video_path)\n",
    "frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n"
     ]
    }
   ],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "out = cv2.VideoWriter(results_path+'Instance Segmentation/pedestrian_1.mp4', fourcc, 20.0, (1920, 1080))\n",
    "for i in range(frames):\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "    success, img = vidcap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    input_tensor = mask_transforms(img)\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    input_batch = input_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "    _, pred_img = get_img(input_batch, output, 0, device, th=0.5)\n",
    "    r = np.array(pred_img)\n",
    "    out.write(r)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
